{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development\n",
    "____\n",
    "\n",
    "This notebook goes into creating supervised model that are used to predict the price of cars. Two models are derrived from this and are saved to be used for future inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Funtion Definitions\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import os\n",
    "import optuna\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataTypes_and_missingValues(df):\n",
    "    info = pd.DataFrame()\n",
    "    info['data_types'] =  df.dtypes\n",
    "    info['unique_values'] = df.nunique()\n",
    "    info['missing_values'] = df.isna().sum()\n",
    "    return info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_split_dataset(path, drop_cols, split=True):\n",
    "    \n",
    "    df = pd.read_csv(path)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    df.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "    if split:\n",
    "        X = df.drop(\"Price\", axis=1)\n",
    "        y = df[\"Price\"]\n",
    "        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        train_df = pd.concat([X_train, y_train], axis=1)\n",
    "        val_df = pd.concat([X_val, y_val], axis=1)\n",
    "        return train_df, val_df\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_numeric_features(X, columns_to_extract):\n",
    "    X_copy = X.copy()\n",
    "    for col in columns_to_extract:\n",
    "        X_copy[col] = pd.to_numeric(X_copy[col].str.split(' ').str[0], downcast='float', errors='coerce')\n",
    "    return X_copy[columns_to_extract]\n",
    "\n",
    "def preprocess_levy_and_fillna(X):\n",
    "\n",
    "    X_copy = X.copy()\n",
    "    X_copy[\"Levy\"].replace(\"-\", None, inplace=True)\n",
    "\n",
    "    X_copy['Levy'] = pd.to_numeric(X_copy['Levy'], errors='coerce')\n",
    "    mean_levy_by_year = X_copy.groupby('Prod. year')['Levy'].mean()\n",
    "    mean_levy_by_year.fillna(0, inplace=True)\n",
    "    \n",
    "    for year in X_copy['Prod. year'].unique():\n",
    "        mask = (X_copy['Prod. year'] == year) & X_copy['Levy'].isnull()\n",
    "        X_copy.loc[mask, 'Levy'] = mean_levy_by_year[year]\n",
    "    \n",
    "    X_copy['Levy'] = X_copy['Levy'].astype(int)\n",
    "    \n",
    "    return X_copy\n",
    "\n",
    "def preprocessing_pipeline(numeric_features_to_extract, ordinal_features, categorical_features, other_features):\n",
    "    \"\"\"This function performs the pre-processing steps for the features and retuns an numeric representation of all the features which is used to train the model\"\"\"\n",
    "    \n",
    "    \n",
    "    column_transformer = make_column_transformer(\n",
    "            (FunctionTransformer(preprocess_levy_and_fillna, validate=False), ['Prod. year', 'Levy']),\n",
    "            (FunctionTransformer(extract_numeric_features, kw_args={'columns_to_extract': numeric_features_to_extract}), numeric_features_to_extract),\n",
    "            (OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1, dtype=int), ordinal_features),\n",
    "            (OneHotEncoder(drop='if_binary', handle_unknown='ignore', sparse_output=False), categorical_features),\n",
    "            (\"drop\", other_features)\n",
    "    )\n",
    "\n",
    "    return column_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trained_model(X_train, y_train, model_algorithm, random_state=124, **model_kwargs):\n",
    "    if 'random_state' in model_algorithm().get_params():\n",
    "        model_kwargs['random_state'] = random_state\n",
    "\n",
    "    model = model_algorithm(**model_kwargs)\n",
    "\n",
    "    numeric_features = X_train.select_dtypes(np.number).columns.to_list()\n",
    "    extract_num_feats = ['Mileage', 'Engine volume']\n",
    "    ord_feats = ['Manufacturer', 'Model', 'Fuel type']\n",
    "    cat_feats = ['Leather interior', 'Gear box type', 'Category']\n",
    "    others = list(set(X_train.columns) - set(numeric_features + extract_num_feats +\n",
    "                                             ['Levy'] + cat_feats + ord_feats))\n",
    "    ct = preprocessing_pipeline(numeric_features_to_extract=extract_num_feats,\n",
    "                                 ordinal_features=ord_feats,\n",
    "                                 categorical_features=cat_feats, other_features=others)\n",
    "\n",
    "    pipeline = make_pipeline(ct, StandardScaler(), model)\n",
    "    pipeline.fit(X_train, y_train.values.ravel())\n",
    "\n",
    "    return pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Pre Processing\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes from the last notebook:\n",
    "\n",
    "1. **Label Encoding:**\n",
    "   - The columns 'Manufacturer' and 'Model' will be label encoded, converting them to numerical representations.\n",
    "\n",
    "2. **Irrelevant Columns:**\n",
    "   - Some columns have been identified as irrelevant and will be dropped. Ideally the decision to drop columns would involve domain expertise or subject matter experts (SMEs).\n",
    "\n",
    "| Dropped Column | Reason                                      |\n",
    "| -------------- | ------------------------------------------- |\n",
    "| Cylinders      | Information is contained in Engine volume, Manufacturer, Model, and Category. |\n",
    "| Drive Wheels   | Similar information is captured elsewhere.   |\n",
    "| Doors          | Considered irrelevant.                       |\n",
    "| Wheel          | Location-specific information.               |\n",
    "| Color          | Deemed irrelevant for the analysis.          |\n",
    "| Airbags        | Considered irrelevant for predicting price. |\n",
    "\n",
    "These changes aim to streamline the dataset by removing redundant or less impactful features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_types</th>\n",
       "      <th>unique_values</th>\n",
       "      <th>missing_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Levy</th>\n",
       "      <td>object</td>\n",
       "      <td>526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manufacturer</th>\n",
       "      <td>object</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <td>object</td>\n",
       "      <td>1392</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prod. year</th>\n",
       "      <td>int64</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <td>object</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leather interior</th>\n",
       "      <td>object</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fuel type</th>\n",
       "      <td>object</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Engine volume</th>\n",
       "      <td>object</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mileage</th>\n",
       "      <td>object</td>\n",
       "      <td>6486</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gear box type</th>\n",
       "      <td>object</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>int64</td>\n",
       "      <td>2062</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 data_types  unique_values  missing_values\n",
       "Levy                 object            526               0\n",
       "Manufacturer         object             64               0\n",
       "Model                object           1392               0\n",
       "Prod. year            int64             50               0\n",
       "Category             object             11               0\n",
       "Leather interior     object              2               0\n",
       "Fuel type            object              7               0\n",
       "Engine volume        object            103               0\n",
       "Mileage              object           6486               0\n",
       "Gear box type        object              4               0\n",
       "Price                 int64           2062               0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the DataFrame-based features\n",
    "dir = os.path.join('.', 'data', 'raw', 'train.csv')\n",
    "drop_cols = ['ID', 'Cylinders', 'Drive wheels', 'Doors', 'Wheel', 'Color', 'Airbags']\n",
    "train_df, val_df = extract_and_split_dataset(dir, drop_cols)\n",
    "get_dataTypes_and_missingValues(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = train_df.select_dtypes(np.number).drop('Price', axis=1).columns.to_list()\n",
    "extract_num_feats = ['Mileage', 'Engine volume']\n",
    "ord_feats = ['Manufacturer', 'Model', 'Fuel type']\n",
    "cat_feats = ['Leather interior','Gear box type', 'Category']\n",
    "others = list(set(train_df.columns) - set(numeric_features + extract_num_feats + ['Levy','Price'] + cat_feats + ord_feats))\n",
    "ct = preprocessing_pipeline(numeric_features_to_extract=extract_num_feats,ordinal_features=ord_feats, \n",
    "                            categorical_features=cat_feats, other_features=others)\n",
    "\n",
    "# remove prices less than $1000\n",
    "train_df = train_df[train_df['Price'] >= 1000]\n",
    "X_train = train_df.drop('Price', axis=1).reset_index(drop=True)\n",
    "y_train = train_df[['Price']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = val_df.drop('Price', axis=1).reset_index(drop=True)\n",
    "y_val = val_df[['Price']].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Linear Models\n",
    "____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg_rmse</th>\n",
       "      <th>train_neg_rmse</th>\n",
       "      <th>test_neg_mae</th>\n",
       "      <th>train_neg_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.170814</td>\n",
       "      <td>0.055032</td>\n",
       "      <td>-29500.578227</td>\n",
       "      <td>-256333.935412</td>\n",
       "      <td>-15500.503915</td>\n",
       "      <td>-17236.019883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.137396</td>\n",
       "      <td>0.055028</td>\n",
       "      <td>-30542.405235</td>\n",
       "      <td>-256334.131923</td>\n",
       "      <td>-14798.064705</td>\n",
       "      <td>-16036.635123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.131754</td>\n",
       "      <td>0.054711</td>\n",
       "      <td>-28643.536914</td>\n",
       "      <td>-256354.815042</td>\n",
       "      <td>-14489.660853</td>\n",
       "      <td>-16568.787232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.133644</td>\n",
       "      <td>0.068409</td>\n",
       "      <td>-31700.611445</td>\n",
       "      <td>-256300.424642</td>\n",
       "      <td>-14097.338817</td>\n",
       "      <td>-15969.625100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.130267</td>\n",
       "      <td>0.057206</td>\n",
       "      <td>-513850.918281</td>\n",
       "      <td>-18373.273789</td>\n",
       "      <td>-20015.595340</td>\n",
       "      <td>-10110.062342</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_neg_rmse  train_neg_rmse  test_neg_mae  \\\n",
       "0  0.170814    0.055032  -29500.578227  -256333.935412 -15500.503915   \n",
       "1  0.137396    0.055028  -30542.405235  -256334.131923 -14798.064705   \n",
       "2  0.131754    0.054711  -28643.536914  -256354.815042 -14489.660853   \n",
       "3  0.133644    0.068409  -31700.611445  -256300.424642 -14097.338817   \n",
       "4  0.130267    0.057206 -513850.918281   -18373.273789 -20015.595340   \n",
       "\n",
       "   train_neg_mae  \n",
       "0  -17236.019883  \n",
       "1  -16036.635123  \n",
       "2  -16568.787232  \n",
       "3  -15969.625100  \n",
       "4  -10110.062342  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scoring = {\n",
    "    'neg_rmse': make_scorer(mean_squared_error, greater_is_better=False, squared=False),\n",
    "    'neg_mae': make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "}\n",
    "\n",
    "pipe = make_pipeline(ct, StandardScaler(), LinearRegression())\n",
    "scores = cross_validate(pipe, X_train, y_train, scoring=scoring, return_train_score=True)\n",
    "cv_df = pd.DataFrame(scores)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg_rmse</th>\n",
       "      <th>train_neg_rmse</th>\n",
       "      <th>test_neg_mae</th>\n",
       "      <th>train_neg_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.127763</td>\n",
       "      <td>0.054871</td>\n",
       "      <td>-29444.796131</td>\n",
       "      <td>-256333.468373</td>\n",
       "      <td>-15456.903065</td>\n",
       "      <td>-17209.919351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.124056</td>\n",
       "      <td>0.056146</td>\n",
       "      <td>-30507.494618</td>\n",
       "      <td>-256334.278342</td>\n",
       "      <td>-14778.701923</td>\n",
       "      <td>-16030.863264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.120629</td>\n",
       "      <td>0.053845</td>\n",
       "      <td>-28657.116339</td>\n",
       "      <td>-256355.269208</td>\n",
       "      <td>-14480.270217</td>\n",
       "      <td>-16571.223276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.134053</td>\n",
       "      <td>0.053781</td>\n",
       "      <td>-31655.347565</td>\n",
       "      <td>-256298.327786</td>\n",
       "      <td>-14106.439712</td>\n",
       "      <td>-16044.255635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.138274</td>\n",
       "      <td>0.130213</td>\n",
       "      <td>-513847.683840</td>\n",
       "      <td>-18375.854170</td>\n",
       "      <td>-20031.247687</td>\n",
       "      <td>-10138.069408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_neg_rmse  train_neg_rmse  test_neg_mae  \\\n",
       "0  0.127763    0.054871  -29444.796131  -256333.468373 -15456.903065   \n",
       "1  0.124056    0.056146  -30507.494618  -256334.278342 -14778.701923   \n",
       "2  0.120629    0.053845  -28657.116339  -256355.269208 -14480.270217   \n",
       "3  0.134053    0.053781  -31655.347565  -256298.327786 -14106.439712   \n",
       "4  0.138274    0.130213 -513847.683840   -18375.854170 -20031.247687   \n",
       "\n",
       "   train_neg_mae  \n",
       "0  -17209.919351  \n",
       "1  -16030.863264  \n",
       "2  -16571.223276  \n",
       "3  -16044.255635  \n",
       "4  -10138.069408  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=1)\n",
    "\n",
    "pipe = make_pipeline(ct, StandardScaler(), poly, LinearRegression())\n",
    "scores = cross_validate(pipe, X_train, y_train, scoring=scoring, return_train_score=True)\n",
    "cv_df = pd.DataFrame(scores)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg_rmse</th>\n",
       "      <th>train_neg_rmse</th>\n",
       "      <th>test_neg_mae</th>\n",
       "      <th>train_neg_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.650070</td>\n",
       "      <td>0.061416</td>\n",
       "      <td>-7.418861e+10</td>\n",
       "      <td>-249882.186484</td>\n",
       "      <td>-1.596250e+09</td>\n",
       "      <td>-19219.697749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.466598</td>\n",
       "      <td>0.129184</td>\n",
       "      <td>-1.659277e+12</td>\n",
       "      <td>-253632.291454</td>\n",
       "      <td>-4.578407e+10</td>\n",
       "      <td>-16669.550168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.489255</td>\n",
       "      <td>0.062922</td>\n",
       "      <td>-8.806342e+05</td>\n",
       "      <td>-252041.672386</td>\n",
       "      <td>-4.269042e+04</td>\n",
       "      <td>-17623.493115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.442200</td>\n",
       "      <td>0.120054</td>\n",
       "      <td>-4.029299e+05</td>\n",
       "      <td>-253020.681497</td>\n",
       "      <td>-2.699216e+04</td>\n",
       "      <td>-16653.150913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.456578</td>\n",
       "      <td>0.060704</td>\n",
       "      <td>-7.427665e+11</td>\n",
       "      <td>-15745.533557</td>\n",
       "      <td>-1.538676e+10</td>\n",
       "      <td>-8857.767338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_neg_rmse  train_neg_rmse  test_neg_mae  \\\n",
       "0  0.650070    0.061416  -7.418861e+10  -249882.186484 -1.596250e+09   \n",
       "1  0.466598    0.129184  -1.659277e+12  -253632.291454 -4.578407e+10   \n",
       "2  0.489255    0.062922  -8.806342e+05  -252041.672386 -4.269042e+04   \n",
       "3  0.442200    0.120054  -4.029299e+05  -253020.681497 -2.699216e+04   \n",
       "4  0.456578    0.060704  -7.427665e+11   -15745.533557 -1.538676e+10   \n",
       "\n",
       "   train_neg_mae  \n",
       "0  -19219.697749  \n",
       "1  -16669.550168  \n",
       "2  -17623.493115  \n",
       "3  -16653.150913  \n",
       "4   -8857.767338  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=2)\n",
    "\n",
    "pipe = make_pipeline(ct, StandardScaler(), poly, LinearRegression())\n",
    "scores = cross_validate(pipe, X_train, y_train, scoring=scoring, return_train_score=True)\n",
    "cv_df = pd.DataFrame(scores)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above it seems like increasing the degree of freedom begins to overfit the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg_rmse</th>\n",
       "      <th>train_neg_rmse</th>\n",
       "      <th>test_neg_mae</th>\n",
       "      <th>train_neg_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.190814</td>\n",
       "      <td>0.049670</td>\n",
       "      <td>-29439.746391</td>\n",
       "      <td>-256333.399409</td>\n",
       "      <td>-15452.107931</td>\n",
       "      <td>-17209.033399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.103515</td>\n",
       "      <td>0.050985</td>\n",
       "      <td>-30510.526030</td>\n",
       "      <td>-256333.665228</td>\n",
       "      <td>-14782.266151</td>\n",
       "      <td>-15995.077318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.168856</td>\n",
       "      <td>0.050008</td>\n",
       "      <td>-28633.293675</td>\n",
       "      <td>-256354.859544</td>\n",
       "      <td>-14486.912158</td>\n",
       "      <td>-16551.543957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.104054</td>\n",
       "      <td>0.050121</td>\n",
       "      <td>-31643.274171</td>\n",
       "      <td>-256297.215706</td>\n",
       "      <td>-14127.277136</td>\n",
       "      <td>-16061.604706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.170362</td>\n",
       "      <td>0.049042</td>\n",
       "      <td>-513851.545448</td>\n",
       "      <td>-18373.351619</td>\n",
       "      <td>-20017.436432</td>\n",
       "      <td>-10112.846938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_neg_rmse  train_neg_rmse  test_neg_mae  \\\n",
       "0  0.190814    0.049670  -29439.746391  -256333.399409 -15452.107931   \n",
       "1  0.103515    0.050985  -30510.526030  -256333.665228 -14782.266151   \n",
       "2  0.168856    0.050008  -28633.293675  -256354.859544 -14486.912158   \n",
       "3  0.104054    0.050121  -31643.274171  -256297.215706 -14127.277136   \n",
       "4  0.170362    0.049042 -513851.545448   -18373.351619 -20017.436432   \n",
       "\n",
       "   train_neg_mae  \n",
       "0  -17209.033399  \n",
       "1  -15995.077318  \n",
       "2  -16551.543957  \n",
       "3  -16061.604706  \n",
       "4  -10112.846938  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(ct, StandardScaler(), Ridge(alpha=0.001))\n",
    "scores = cross_validate(pipe, X_train, y_train, scoring=scoring, return_train_score=True)\n",
    "cv_df = pd.DataFrame(scores)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above showcases a feature of linear regression related to regularization. Lasso or ElasticNet are recommended approaches when dealing with datasets that have a substantial number of features, as is the case here. It's important to note that Lasso, while effective, may exhibit a longer convergence time compared to ElasticNet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.car-prices-prediction/lib64/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.419e+14, tolerance: 6.937e+10\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/ec2-user/.car-prices-prediction/lib64/python3.9/site-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.797e+11, tolerance: 2.686e+08\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg_rmse</th>\n",
       "      <th>train_neg_rmse</th>\n",
       "      <th>test_neg_mae</th>\n",
       "      <th>train_neg_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.319381</td>\n",
       "      <td>0.076683</td>\n",
       "      <td>-42649.060491</td>\n",
       "      <td>-323110.245312</td>\n",
       "      <td>-18967.359772</td>\n",
       "      <td>-22267.826145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.258117</td>\n",
       "      <td>0.075128</td>\n",
       "      <td>-325275.926814</td>\n",
       "      <td>-17293.267563</td>\n",
       "      <td>-14081.723794</td>\n",
       "      <td>-10070.759702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_neg_rmse  train_neg_rmse  test_neg_mae  \\\n",
       "0  0.319381    0.076683  -42649.060491  -323110.245312 -18967.359772   \n",
       "1  0.258117    0.075128 -325275.926814   -17293.267563 -14081.723794   \n",
       "\n",
       "   train_neg_mae  \n",
       "0  -22267.826145  \n",
       "1  -10070.759702  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(ct, StandardScaler(), ElasticNet(alpha=0.001))\n",
    "scores = cross_validate(pipe, X_train, y_train, cv=2, return_train_score=True, scoring=scoring)\n",
    "cv_df = pd.DataFrame(scores)\n",
    "cv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Ensemble Techniques\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The linear models exhibit sub-optimal performance,thet ensemble techniques might yield better results.\n",
    "* Utilizing a Random Forest Regressor for bagging.\n",
    "* \n",
    "Employing an XGBoost Regressor for boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--> Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg_rmse</th>\n",
       "      <th>train_neg_rmse</th>\n",
       "      <th>test_neg_mae</th>\n",
       "      <th>train_neg_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.009693</td>\n",
       "      <td>0.132792</td>\n",
       "      <td>-343672.667973</td>\n",
       "      <td>-125943.870059</td>\n",
       "      <td>-17065.023470</td>\n",
       "      <td>-4259.494996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.124063</td>\n",
       "      <td>0.135222</td>\n",
       "      <td>-16703.607478</td>\n",
       "      <td>-127674.795976</td>\n",
       "      <td>-5856.140884</td>\n",
       "      <td>-4606.214561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.824773</td>\n",
       "      <td>0.133104</td>\n",
       "      <td>-10332.761630</td>\n",
       "      <td>-108754.223953</td>\n",
       "      <td>-5179.019694</td>\n",
       "      <td>-4613.119585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.797619</td>\n",
       "      <td>0.134260</td>\n",
       "      <td>-20543.212985</td>\n",
       "      <td>-100762.600004</td>\n",
       "      <td>-5490.103241</td>\n",
       "      <td>-4749.378293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.584707</td>\n",
       "      <td>0.131690</td>\n",
       "      <td>-513664.199830</td>\n",
       "      <td>-5194.255601</td>\n",
       "      <td>-15048.202796</td>\n",
       "      <td>-2042.589494</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_neg_rmse  train_neg_rmse  test_neg_mae  \\\n",
       "0  5.009693    0.132792 -343672.667973  -125943.870059 -17065.023470   \n",
       "1  5.124063    0.135222  -16703.607478  -127674.795976  -5856.140884   \n",
       "2  4.824773    0.133104  -10332.761630  -108754.223953  -5179.019694   \n",
       "3  4.797619    0.134260  -20543.212985  -100762.600004  -5490.103241   \n",
       "4  4.584707    0.131690 -513664.199830    -5194.255601 -15048.202796   \n",
       "\n",
       "   train_neg_mae  \n",
       "0   -4259.494996  \n",
       "1   -4606.214561  \n",
       "2   -4613.119585  \n",
       "3   -4749.378293  \n",
       "4   -2042.589494  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_pipe = make_pipeline(ct, StandardScaler(), RandomForestRegressor())\n",
    "rf_scores = cross_validate(rf_pipe, X_train, y_train.values.ravel(), scoring=scoring, return_train_score=True)\n",
    "rf_df = pd.DataFrame(rf_scores)\n",
    "rf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root Mean Squared Error (RMSE) is more sensitive to outliers compared to Mean Absolute Error (MAE), as evidenced by the broader range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- Default scoring is 'neg_mean_squared_error' for Mean Squared Error (MSE) -->--> XGB Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fit_time</th>\n",
       "      <th>score_time</th>\n",
       "      <th>test_neg_rmse</th>\n",
       "      <th>train_neg_rmse</th>\n",
       "      <th>test_neg_mae</th>\n",
       "      <th>train_neg_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.847337</td>\n",
       "      <td>0.061041</td>\n",
       "      <td>-509996.129713</td>\n",
       "      <td>-5484.398557</td>\n",
       "      <td>-15749.847274</td>\n",
       "      <td>-3646.019321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.536711</td>\n",
       "      <td>0.062022</td>\n",
       "      <td>-11854.437565</td>\n",
       "      <td>-5438.273834</td>\n",
       "      <td>-5989.564691</td>\n",
       "      <td>-3596.556270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.489095</td>\n",
       "      <td>0.131222</td>\n",
       "      <td>-11194.303958</td>\n",
       "      <td>-5523.512809</td>\n",
       "      <td>-6008.727843</td>\n",
       "      <td>-3681.244221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.552229</td>\n",
       "      <td>0.092416</td>\n",
       "      <td>-16897.649489</td>\n",
       "      <td>-5902.424604</td>\n",
       "      <td>-5895.344839</td>\n",
       "      <td>-3868.452781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.544082</td>\n",
       "      <td>0.116141</td>\n",
       "      <td>-513647.674401</td>\n",
       "      <td>-5244.705145</td>\n",
       "      <td>-15628.569695</td>\n",
       "      <td>-3416.997986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   fit_time  score_time  test_neg_rmse  train_neg_rmse  test_neg_mae  \\\n",
       "0  1.847337    0.061041 -509996.129713    -5484.398557 -15749.847274   \n",
       "1  1.536711    0.062022  -11854.437565    -5438.273834  -5989.564691   \n",
       "2  1.489095    0.131222  -11194.303958    -5523.512809  -6008.727843   \n",
       "3  1.552229    0.092416  -16897.649489    -5902.424604  -5895.344839   \n",
       "4  1.544082    0.116141 -513647.674401    -5244.705145 -15628.569695   \n",
       "\n",
       "   train_neg_mae  \n",
       "0   -3646.019321  \n",
       "1   -3596.556270  \n",
       "2   -3681.244221  \n",
       "3   -3868.452781  \n",
       "4   -3416.997986  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_pipe = make_pipeline(ct, StandardScaler(), XGBRegressor())\n",
    "xgb_scores = cross_validate(xgb_pipe, X_train, y_train, scoring=scoring, return_train_score=True)\n",
    "xgb_df = pd.DataFrame(xgb_scores)\n",
    "xgb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on these values the Random Forest Regressor seems to perform better than the XGB Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Hyper-parameter Tuning\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### --> Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-27 00:51:25,845] A new study created in memory with name: no-name-0fc2da14-aa35-4bb5-8632-2a6bf74ebdd2\n",
      "[I 2023-12-27 00:51:27,878] Trial 0 finished with value: 11994.321184127182 and parameters: {'n_estimators': 241, 'max_depth': 32, 'min_samples_split': 0.002621027568944447, 'min_samples_leaf': 0.06906094000938462, 'max_features': 0.525436010262137, 'bootstrap': True}. Best is trial 0 with value: 11994.321184127182.\n",
      "[I 2023-12-27 00:51:28,460] Trial 1 finished with value: 10306.236900896882 and parameters: {'n_estimators': 48, 'max_depth': 31, 'min_samples_split': 0.0042638078426972216, 'min_samples_leaf': 0.002118996252061989, 'max_features': 0.16866457400703438, 'bootstrap': True}. Best is trial 1 with value: 10306.236900896882.\n",
      "[I 2023-12-27 00:51:30,422] Trial 2 pruned. \n",
      "[I 2023-12-27 00:51:31,839] Trial 3 pruned. \n",
      "[I 2023-12-27 00:51:35,733] Trial 4 pruned. \n",
      "[I 2023-12-27 00:51:36,156] Trial 5 pruned. \n",
      "[I 2023-12-27 00:51:46,089] Trial 6 pruned. \n",
      "[I 2023-12-27 00:51:46,761] Trial 7 pruned. \n",
      "[I 2023-12-27 00:51:51,941] Trial 8 pruned. \n",
      "[I 2023-12-27 00:51:56,758] Trial 9 finished with value: 10897.037244158455 and parameters: {'n_estimators': 388, 'max_depth': 7, 'min_samples_split': 0.004769590920954828, 'min_samples_leaf': 0.009177733649861068, 'max_features': 0.332698613362437, 'bootstrap': False}. Best is trial 1 with value: 10306.236900896882.\n",
      "[I 2023-12-27 00:52:14,353] Trial 10 finished with value: 9567.583024310556 and parameters: {'n_estimators': 368, 'max_depth': 32, 'min_samples_split': 0.011088573858126683, 'min_samples_leaf': 0.0011377928422771372, 'max_features': 0.9704512608937852, 'bootstrap': False}. Best is trial 10 with value: 9567.583024310556.\n",
      "[I 2023-12-27 00:52:31,493] Trial 11 finished with value: 9446.582709549228 and parameters: {'n_estimators': 355, 'max_depth': 32, 'min_samples_split': 0.009244257274724367, 'min_samples_leaf': 0.0012301267176291545, 'max_features': 0.9774534942873143, 'bootstrap': False}. Best is trial 11 with value: 9446.582709549228.\n",
      "[I 2023-12-27 00:52:51,326] Trial 12 finished with value: 9367.287070308472 and parameters: {'n_estimators': 401, 'max_depth': 29, 'min_samples_split': 0.00859147449162026, 'min_samples_leaf': 0.0010003533374873263, 'max_features': 0.9800273521027659, 'bootstrap': False}. Best is trial 12 with value: 9367.287070308472.\n",
      "[I 2023-12-27 00:53:18,471] Trial 13 finished with value: 8426.938458987303 and parameters: {'n_estimators': 492, 'max_depth': 26, 'min_samples_split': 0.001237437439433271, 'min_samples_leaf': 0.0010012025005034363, 'max_features': 0.9939935470791209, 'bootstrap': False}. Best is trial 13 with value: 8426.938458987303.\n",
      "[I 2023-12-27 00:53:40,929] Trial 14 pruned. \n",
      "[I 2023-12-27 00:54:03,641] Trial 15 pruned. \n",
      "[I 2023-12-27 00:54:11,878] Trial 16 pruned. \n",
      "[I 2023-12-27 00:54:35,086] Trial 17 pruned. \n",
      "[I 2023-12-27 00:54:50,138] Trial 18 pruned. \n",
      "[I 2023-12-27 00:54:55,457] Trial 19 pruned. \n",
      "[I 2023-12-27 00:55:39,093] Trial 20 pruned. \n",
      "[I 2023-12-27 00:55:55,603] Trial 21 pruned. \n",
      "[I 2023-12-27 00:56:15,696] Trial 22 pruned. \n",
      "[I 2023-12-27 00:56:30,096] Trial 23 pruned. \n",
      "[I 2023-12-27 00:56:49,269] Trial 24 pruned. \n",
      "[I 2023-12-27 00:56:57,148] Trial 25 pruned. \n",
      "[I 2023-12-27 00:57:26,346] Trial 26 pruned. \n",
      "[I 2023-12-27 00:57:57,868] Trial 27 pruned. \n",
      "[I 2023-12-27 00:58:18,683] Trial 28 pruned. \n",
      "[I 2023-12-27 00:58:41,985] Trial 29 pruned. \n",
      "[I 2023-12-27 00:58:47,173] Trial 30 pruned. \n",
      "[I 2023-12-27 00:59:03,147] Trial 31 pruned. \n",
      "[I 2023-12-27 00:59:20,947] Trial 32 pruned. \n",
      "[I 2023-12-27 00:59:34,270] Trial 33 pruned. \n",
      "[I 2023-12-27 00:59:49,717] Trial 34 pruned. \n",
      "[I 2023-12-27 01:00:04,288] Trial 35 pruned. \n",
      "[I 2023-12-27 01:00:10,639] Trial 36 pruned. \n",
      "[I 2023-12-27 01:00:23,628] Trial 37 pruned. \n",
      "[I 2023-12-27 01:00:45,065] Trial 38 pruned. \n",
      "[I 2023-12-27 01:00:57,098] Trial 39 finished with value: 8575.12238683431 and parameters: {'n_estimators': 267, 'max_depth': 16, 'min_samples_split': 0.0033359202220753434, 'min_samples_leaf': 0.001001177683773987, 'max_features': 0.8493801001622486, 'bootstrap': False}. Best is trial 13 with value: 8426.938458987303.\n",
      "[I 2023-12-27 01:00:57,904] Trial 40 pruned. \n",
      "[I 2023-12-27 01:01:10,406] Trial 41 pruned. \n",
      "[I 2023-12-27 01:01:29,023] Trial 42 pruned. \n",
      "[I 2023-12-27 01:01:46,158] Trial 43 pruned. \n",
      "[I 2023-12-27 01:02:03,130] Trial 44 finished with value: 8509.805011203738 and parameters: {'n_estimators': 316, 'max_depth': 17, 'min_samples_split': 0.002236635210595308, 'min_samples_leaf': 0.0010327958464959788, 'max_features': 0.9979657554904765, 'bootstrap': False}. Best is trial 13 with value: 8426.938458987303.\n",
      "[I 2023-12-27 01:02:13,583] Trial 45 pruned. \n",
      "[I 2023-12-27 01:02:27,119] Trial 46 pruned. \n",
      "[I 2023-12-27 01:02:32,725] Trial 47 pruned. \n",
      "[I 2023-12-27 01:02:37,250] Trial 48 pruned. \n",
      "[I 2023-12-27 01:02:42,462] Trial 49 finished with value: 8323.113058519546 and parameters: {'n_estimators': 97, 'max_depth': 22, 'min_samples_split': 0.0023146536832538587, 'min_samples_leaf': 0.001002134328961941, 'max_features': 0.9414935783510276, 'bootstrap': False}. Best is trial 49 with value: 8323.113058519546.\n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "    n_estimators = trial.suggest_int('n_estimators', 10, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 1, 32)\n",
    "    min_samples_split = trial.suggest_float('min_samples_split', 0.001, 0.5, log=True)\n",
    "    min_samples_leaf = trial.suggest_float('min_samples_leaf', 0.001, 0.5, log=True)\n",
    "    max_features = trial.suggest_float('max_features', 0.1, 1.0)\n",
    "    bootstrap = trial.suggest_categorical('bootstrap', [True, False])\n",
    "\n",
    "    model_kwargs = {'n_estimators': n_estimators,\n",
    "                    'max_depth': max_depth,\n",
    "                    'min_samples_split': min_samples_split,\n",
    "                    'min_samples_leaf': min_samples_leaf,\n",
    "                    'max_features': max_features,\n",
    "                    'bootstrap': bootstrap,\n",
    "                    'n_jobs':-1\n",
    "                    }\n",
    "\n",
    "    model = get_trained_model(X_train, y_train, RandomForestRegressor, **model_kwargs)\n",
    "    \n",
    "    y_pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "    trial.report(mae, step=trial.number)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()    \n",
    "\n",
    "    return mae\n",
    "\n",
    "rf_study = optuna.create_study(direction='minimize', pruner=optuna.pruners.SuccessiveHalvingPruner())\n",
    "rf_study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Trial:\n",
      "Value:  8323.113058519546\n",
      "Params: \n",
      "    n_estimators: 97\n",
      "    max_depth: 22\n",
      "    min_samples_split: 0.0023146536832538587\n",
      "    min_samples_leaf: 0.001002134328961941\n",
      "    max_features: 0.9414935783510276\n",
      "    bootstrap: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Trial:\")\n",
    "rf_best_trial = rf_study.best_trial\n",
    "print(\"Value: \", rf_best_trial.value)\n",
    "print(\"Params: \")\n",
    "for key, value in rf_best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trail above takes roughly 15 minutes going through 50 trials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### -->XGB Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-27 01:03:39,874] A new study created in memory with name: no-name-7608d87f-953a-4f8c-a6af-91cac0e6c404\n",
      "[I 2023-12-27 01:03:55,425] Trial 0 finished with value: 16423.08538298525 and parameters: {'colsample_bytree': 0.5159720641992551, 'min_child_weight': 6.5345431720916825, 'learning_rate': 0.025034979032681065, 'gamma': 0.36085100328987973, 'max_depth': 9, 'n_estimators': 891, 'subsample': 0.9575718682495609, 'reg_alpha': 0.759695659812304, 'reg_lambda': 0.017177658357771408}. Best is trial 0 with value: 16423.08538298525.\n",
      "[I 2023-12-27 01:03:57,883] Trial 1 finished with value: 21547.743247895134 and parameters: {'colsample_bytree': 0.643528101849511, 'min_child_weight': 8.411085664282417, 'learning_rate': 0.23842487432007137, 'gamma': 0.9229116951102323, 'max_depth': 7, 'n_estimators': 128, 'subsample': 0.5464639402097278, 'reg_alpha': 0.4813666619761755, 'reg_lambda': 0.3351180798715955}. Best is trial 0 with value: 16423.08538298525.\n",
      "[I 2023-12-27 01:04:10,828] Trial 2 finished with value: 17613.096330793374 and parameters: {'colsample_bytree': 0.6592827381612576, 'min_child_weight': 4.00248065852473, 'learning_rate': 0.15924498637249068, 'gamma': 0.15689810630068635, 'max_depth': 9, 'n_estimators': 601, 'subsample': 0.8551148814637832, 'reg_alpha': 0.3024225123615022, 'reg_lambda': 0.22829872152131414}. Best is trial 0 with value: 16423.08538298525.\n",
      "[I 2023-12-27 01:04:21,555] Trial 3 finished with value: 13552.566241817904 and parameters: {'colsample_bytree': 0.9765693748520133, 'min_child_weight': 1.9483541647612106, 'learning_rate': 0.15086264340336616, 'gamma': 0.5207874808513598, 'max_depth': 4, 'n_estimators': 834, 'subsample': 0.7274079523140519, 'reg_alpha': 0.19992052704976115, 'reg_lambda': 0.5709058879707676}. Best is trial 3 with value: 13552.566241817904.\n",
      "[I 2023-12-27 01:04:33,949] Trial 4 pruned. \n",
      "[I 2023-12-27 01:04:37,735] Trial 5 pruned. \n",
      "[I 2023-12-27 01:04:42,587] Trial 6 pruned. \n",
      "[I 2023-12-27 01:04:57,293] Trial 7 pruned. \n",
      "[I 2023-12-27 01:05:01,219] Trial 8 pruned. \n",
      "[I 2023-12-27 01:05:19,777] Trial 9 pruned. \n",
      "[I 2023-12-27 01:05:27,004] Trial 10 pruned. \n",
      "[I 2023-12-27 01:05:36,092] Trial 11 pruned. \n",
      "[I 2023-12-27 01:05:49,012] Trial 12 finished with value: 15614.385205677794 and parameters: {'colsample_bytree': 0.8583641269452115, 'min_child_weight': 6.102644472832069, 'learning_rate': 0.07090081854279537, 'gamma': 0.4018888156411367, 'max_depth': 6, 'n_estimators': 799, 'subsample': 0.8309343033146148, 'reg_alpha': 0.6378252735399647, 'reg_lambda': 0.4601128397916184}. Best is trial 3 with value: 13552.566241817904.\n",
      "[I 2023-12-27 01:06:01,766] Trial 13 finished with value: 13801.120654667762 and parameters: {'colsample_bytree': 0.8824484780892087, 'min_child_weight': 4.962575479163676, 'learning_rate': 0.08752802381531628, 'gamma': 0.582574193716427, 'max_depth': 6, 'n_estimators': 753, 'subsample': 0.8276015221956858, 'reg_alpha': 0.5850672062700311, 'reg_lambda': 0.6278411904433887}. Best is trial 3 with value: 13552.566241817904.\n",
      "[I 2023-12-27 01:06:11,176] Trial 14 pruned. \n",
      "[I 2023-12-27 01:06:20,029] Trial 15 pruned. \n",
      "[I 2023-12-27 01:06:30,418] Trial 16 pruned. \n",
      "[I 2023-12-27 01:06:39,082] Trial 17 finished with value: 13798.020272067233 and parameters: {'colsample_bytree': 0.9202671514593954, 'min_child_weight': 1.7134008520473305, 'learning_rate': 0.06813139769619561, 'gamma': 0.605098186932383, 'max_depth': 4, 'n_estimators': 691, 'subsample': 0.6923811240687666, 'reg_alpha': 0.5316659885868091, 'reg_lambda': 0.8372124798848779}. Best is trial 3 with value: 13552.566241817904.\n",
      "[I 2023-12-27 01:06:51,661] Trial 18 pruned. \n",
      "[I 2023-12-27 01:06:56,428] Trial 19 pruned. \n",
      "[I 2023-12-27 01:07:11,336] Trial 20 finished with value: 12363.822555947147 and parameters: {'colsample_bytree': 0.9346008566577801, 'min_child_weight': 2.349745581071975, 'learning_rate': 0.13538891224649321, 'gamma': 0.9953070671712433, 'max_depth': 7, 'n_estimators': 674, 'subsample': 0.7491837239448105, 'reg_alpha': 0.517712928293185, 'reg_lambda': 0.6940556467567488}. Best is trial 20 with value: 12363.822555947147.\n",
      "[I 2023-12-27 01:07:26,715] Trial 21 pruned. \n",
      "[I 2023-12-27 01:07:40,086] Trial 22 pruned. \n",
      "[I 2023-12-27 01:07:51,524] Trial 23 pruned. \n",
      "[I 2023-12-27 01:08:13,433] Trial 24 finished with value: 11247.888257920884 and parameters: {'colsample_bytree': 0.9401890476935562, 'min_child_weight': 1.0220081538550025, 'learning_rate': 0.059989859744333074, 'gamma': 0.8163077655117942, 'max_depth': 8, 'n_estimators': 846, 'subsample': 0.7625083417197541, 'reg_alpha': 0.30409875363206856, 'reg_lambda': 0.671501431926757}. Best is trial 24 with value: 11247.888257920884.\n",
      "[I 2023-12-27 01:08:36,022] Trial 25 finished with value: 10335.160944419871 and parameters: {'colsample_bytree': 0.9646732508032053, 'min_child_weight': 1.1571942626519909, 'learning_rate': 0.13727700713379737, 'gamma': 0.8533555037383329, 'max_depth': 8, 'n_estimators': 847, 'subsample': 0.772297522878517, 'reg_alpha': 0.2458851729438928, 'reg_lambda': 0.47673815610305326}. Best is trial 25 with value: 10335.160944419871.\n",
      "[I 2023-12-27 01:09:00,122] Trial 26 finished with value: 9982.015705099571 and parameters: {'colsample_bytree': 0.9434789563415659, 'min_child_weight': 1.0358167272839778, 'learning_rate': 0.09598087080650966, 'gamma': 0.8356065338531264, 'max_depth': 8, 'n_estimators': 923, 'subsample': 0.7754271373333944, 'reg_alpha': 0.32713906007040405, 'reg_lambda': 0.4679755202803385}. Best is trial 26 with value: 9982.015705099571.\n",
      "[I 2023-12-27 01:09:24,085] Trial 27 pruned. \n",
      "[I 2023-12-27 01:09:46,228] Trial 28 pruned. \n",
      "[I 2023-12-27 01:10:12,686] Trial 29 pruned. \n",
      "[I 2023-12-27 01:10:37,171] Trial 30 pruned. \n",
      "[I 2023-12-27 01:11:01,291] Trial 31 pruned. \n",
      "[I 2023-12-27 01:11:20,973] Trial 32 pruned. \n",
      "[I 2023-12-27 01:11:41,138] Trial 33 finished with value: 9922.72418868167 and parameters: {'colsample_bytree': 0.9631957317565838, 'min_child_weight': 1.8584570641167972, 'learning_rate': 0.06197672428077236, 'gamma': 0.8957130936860205, 'max_depth': 8, 'n_estimators': 773, 'subsample': 0.8086460330592216, 'reg_alpha': 0.42778818316669115, 'reg_lambda': 0.3932721406305125}. Best is trial 33 with value: 9922.72418868167.\n",
      "[I 2023-12-27 01:12:00,430] Trial 34 finished with value: 9936.388306873918 and parameters: {'colsample_bytree': 0.9677964245298915, 'min_child_weight': 1.79909887176232, 'learning_rate': 0.05974509972087179, 'gamma': 0.8926371972573738, 'max_depth': 8, 'n_estimators': 754, 'subsample': 0.8766952017000589, 'reg_alpha': 0.29889328168838086, 'reg_lambda': 0.3824959872059577}. Best is trial 33 with value: 9922.72418868167.\n",
      "[I 2023-12-27 01:12:22,580] Trial 35 pruned. \n",
      "[I 2023-12-27 01:12:27,394] Trial 36 finished with value: 9929.824057246704 and parameters: {'colsample_bytree': 0.9970244409431903, 'min_child_weight': 1.770097183149818, 'learning_rate': 0.03282570375246768, 'gamma': 0.9337280167008819, 'max_depth': 8, 'n_estimators': 176, 'subsample': 0.8718998910990498, 'reg_alpha': 0.26083561393072346, 'reg_lambda': 0.3150486959741898}. Best is trial 33 with value: 9922.72418868167.\n",
      "[I 2023-12-27 01:12:32,533] Trial 37 finished with value: 9528.140657428454 and parameters: {'colsample_bytree': 0.9869558487270543, 'min_child_weight': 3.4845262398526113, 'learning_rate': 0.03233028587193833, 'gamma': 0.9298299473259279, 'max_depth': 10, 'n_estimators': 148, 'subsample': 0.8897952625824946, 'reg_alpha': 0.4454441282066663, 'reg_lambda': 0.3267434055752029}. Best is trial 37 with value: 9528.140657428454.\n",
      "[I 2023-12-27 01:12:36,674] Trial 38 pruned. \n",
      "[I 2023-12-27 01:12:44,812] Trial 39 pruned. \n",
      "[I 2023-12-27 01:12:49,898] Trial 40 pruned. \n",
      "[I 2023-12-27 01:12:55,623] Trial 41 pruned. \n",
      "[I 2023-12-27 01:13:05,174] Trial 42 pruned. \n",
      "[I 2023-12-27 01:14:21,901] Trial 43 pruned. \n",
      "[I 2023-12-27 01:15:16,672] Trial 44 finished with value: 9810.637091411152 and parameters: {'colsample_bytree': 0.975153532689112, 'min_child_weight': 1.58118303931032, 'learning_rate': 0.05855821849088796, 'gamma': 0.789777766437757, 'max_depth': 9, 'n_estimators': 407, 'subsample': 0.8202397315209073, 'reg_alpha': 0.45711721135583394, 'reg_lambda': 0.3320966251952175}. Best is trial 37 with value: 9528.140657428454.\n",
      "[I 2023-12-27 01:16:33,437] Trial 45 pruned. \n",
      "[I 2023-12-27 01:19:02,659] Trial 46 finished with value: 9174.03524193707 and parameters: {'colsample_bytree': 0.9992411270180139, 'min_child_weight': 1.5465167813377119, 'learning_rate': 0.022105136022093186, 'gamma': 0.7112118612067738, 'max_depth': 10, 'n_estimators': 383, 'subsample': 0.8834739784801448, 'reg_alpha': 0.42621100303390186, 'reg_lambda': 0.214096062390204}. Best is trial 46 with value: 9174.03524193707.\n",
      "[I 2023-12-27 01:21:26,512] Trial 47 finished with value: 9045.705606913598 and parameters: {'colsample_bytree': 0.9976711378207281, 'min_child_weight': 3.157431071502319, 'learning_rate': 0.01094008867924328, 'gamma': 0.7032856276348437, 'max_depth': 10, 'n_estimators': 351, 'subsample': 0.9118162108383395, 'reg_alpha': 0.43473034704658026, 'reg_lambda': 0.2305809419908718}. Best is trial 47 with value: 9045.705606913598.\n",
      "[I 2023-12-27 01:23:32,183] Trial 48 pruned. \n",
      "[I 2023-12-27 01:26:25,880] Trial 49 pruned. \n"
     ]
    }
   ],
   "source": [
    "def objective(trial):\n",
    "\n",
    "    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "    min_child_weight = trial.suggest_float('min_child_weight', 1, 10)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3)\n",
    "    gamma = trial.suggest_float('gamma', 0.0, 1.0)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 10)\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    subsample = trial.suggest_float('subsample', 0.5, 1.0)\n",
    "    reg_alpha = trial.suggest_float('reg_alpha', 0.0, 1.0)\n",
    "    reg_lambda = trial.suggest_float('reg_lambda', 0.0, 1.0)\n",
    "\n",
    "    model_kwargs = {\n",
    "                    'colsample_bytree': colsample_bytree,\n",
    "                    'min_child_weight': min_child_weight,\n",
    "                    'learning_rate': learning_rate,\n",
    "                    'gamma': gamma,\n",
    "                    'max_depth': max_depth,\n",
    "                    'n_estimators': n_estimators,\n",
    "                    'subsample': subsample,\n",
    "                    'reg_alpha': reg_alpha,\n",
    "                    'reg_lambda': reg_lambda,\n",
    "                    }\n",
    "\n",
    "    model = get_trained_model(X_train, y_train, XGBRegressor, **model_kwargs)\n",
    "\n",
    "    y_pred = model.predict(X_val)\n",
    "    mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "    trial.report(mae, step=trial.number)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()   \n",
    "    \n",
    "    return mae\n",
    "\n",
    "xgb_study = optuna.create_study(direction='minimize', pruner=optuna.pruners.SuccessiveHalvingPruner())\n",
    "xgb_study.optimize(objective, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Trial:\n",
      "Value:  9045.705606913598\n",
      "Params: \n",
      "    colsample_bytree: 0.9976711378207281\n",
      "    min_child_weight: 3.157431071502319\n",
      "    learning_rate: 0.01094008867924328\n",
      "    gamma: 0.7032856276348437\n",
      "    max_depth: 10\n",
      "    n_estimators: 351\n",
      "    subsample: 0.9118162108383395\n",
      "    reg_alpha: 0.43473034704658026\n",
      "    reg_lambda: 0.2305809419908718\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Trial:\")\n",
    "xgb_best_trial = xgb_study.best_trial\n",
    "print(\"Value: \", xgb_best_trial.value)\n",
    "print(\"Params: \")\n",
    "for key, value in xgb_best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Regressor yielded a superior best trial compared to the XGBoost Regressor. However, it's noteworthy that when I conducted this experiment locally, as opposed to on AWS Cloud9 where it's currently running, the XGBoost Regressor exhibited faster runtime and superior results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Saving the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retraining the ensemble models with the best hyper parameters and saving these models and the transformer in a file so that they can be reused."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, filename_prefix):\n",
    "    \"\"\"\n",
    "    Save a machine learning model along with the current date and version.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The machine learning model to be saved.\n",
    "    - filename_prefix: A prefix for the filename indicating the type of model.\n",
    "\n",
    "    Returns:\n",
    "    - filename: The filename of the saved model.\n",
    "    \"\"\"\n",
    "    date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    version = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    filename = f'./models/{filename_prefix}_v{version}.joblib'\n",
    "\n",
    "    # save file\n",
    "    if not os.path.exists('./models'):\n",
    "        os.makedirs('./models')\n",
    "    joblib.dump({'model': model, 'date': date, 'version': version}, filename)\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_save_model(X_train, y_train, model_class, **params):\n",
    "    \"\"\"\n",
    "    Train a machine learning model, save it along with the current date and version, and return the filename.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: The training features.\n",
    "    - y_train: The training labels.\n",
    "    - model_class: The class of the machine learning model to be trained.\n",
    "    - filename_prefix: A prefix for the filename indicating the type of model.\n",
    "    - **params: Additional parameters to be passed to the model during initialization.\n",
    "\n",
    "    Returns:\n",
    "    - filename: The filename of the saved model.\n",
    "    \"\"\"\n",
    "    \n",
    "    model = get_trained_model(X_train, y_train, model_class, **params)\n",
    "    filename_prefix = model_class.__name__\n",
    "\n",
    "    filename = save_model(model, filename_prefix)\n",
    "\n",
    "    return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'n_estimators': 97,\n",
       "  'max_depth': 22,\n",
       "  'min_samples_split': 0.0023146536832538587,\n",
       "  'min_samples_leaf': 0.001002134328961941,\n",
       "  'max_features': 0.9414935783510276,\n",
       "  'bootstrap': False},\n",
       " {'colsample_bytree': 0.9976711378207281,\n",
       "  'min_child_weight': 3.157431071502319,\n",
       "  'learning_rate': 0.01094008867924328,\n",
       "  'gamma': 0.7032856276348437,\n",
       "  'max_depth': 10,\n",
       "  'n_estimators': 351,\n",
       "  'subsample': 0.9118162108383395,\n",
       "  'reg_alpha': 0.43473034704658026,\n",
       "  'reg_lambda': 0.2305809419908718})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_best_trial.params, xgb_best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_filename = train_and_save_model(X_train, y_train, RandomForestRegressor, **rf_best_trial.params)\n",
    "xgb_filename = train_and_save_model(X_train, y_train, XGBRegressor, **xgb_best_trial.params)\n",
    "ct_filename = save_model(ct, 'columntransformer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Currently, there is no baseline model in production; the current model represents the best available option.\n",
    "  \n",
    "- To improve model performance, further optimizations and feature engineering can be explored to create more optimal models.\n",
    "\n",
    "- The next step involves saving these models for reuse. In future notebooks, a model registry will be implemented to store various models, facilitating the selection of optimal models.\n",
    "\n",
    "- Deployment plans are underway to make these models accessible for predicting car prices. Once deployed, users will be able to input data, and the model will calculate the corresponding car price.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
